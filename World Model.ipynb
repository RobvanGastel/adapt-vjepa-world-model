{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c458b5f6-233b-4fb1-8399-7414f6e47454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import repeat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from video_jepa.data import PendulumDataset\n",
    "from video_jepa.world_model import WorldModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbc8a6-099d-41d6-a068-dba0eac130b3",
   "metadata": {},
   "source": [
    "video_encoder, _ = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_vit_large')\n",
    "\n",
    "n_frames = 3\n",
    "input_size = (128, 128)\n",
    "action_embed_dim = 96\n",
    "patch_size = video_encoder.patch_size\n",
    "tubelet_size = video_encoder.tubelet_size\n",
    "model = WorldModel(\n",
    "    num_hist=n_frames,\n",
    "    num_pred=n_frames,\n",
    "    video_encoder=video_encoder,\n",
    "    input_size=input_size,\n",
    "    action_dim=1,\n",
    "    action_embed_dim=96,\n",
    ")\n",
    "\n",
    "# Load pretrained models\n",
    "model.latent_predictor.load_state_dict(torch.load(\"output/latent_predictor.pt\"))\n",
    "model.decoder.load_state_dict(torch.load(\"output/decoder.pt\"))\n",
    "model.action_encoder.load_state_dict(torch.load(\"output/action_emb.pt\"))\n",
    "model.cuda()\n",
    "\n",
    "# Predicting future frames\n",
    "model.latent_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4a288-e887-4061-b660-bbe29e65ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training set to visualize next frame predictions\n",
    "train_dataset = PendulumDataset(\n",
    "    seq_len=12,\n",
    "    input_size=(128, 128),\n",
    "    include_states=False,\n",
    "    include_actions=True\n",
    ")\n",
    "\n",
    "# Sample an index from the training set\n",
    "batch = train_dataset[1]\n",
    "x = batch[\"video\"].unsqueeze(0).moveaxis(1, 2).cuda()\n",
    "actions = batch[\"actions\"].unsqueeze(0).cuda()\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61fb41-ecc7-4efd-9424-58c91ebcff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The video looks as following\n",
    "vid = x[0].cpu().permute(1, 0, 2, 3)  # (12, 3, H, W)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 10))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(vid[i].moveaxis(0, -1).cpu().numpy())\n",
    "    ax.set_title(f\"t={i}\", fontsize=10)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30544058-5238-46ba-b7f1-b4a941d5219a",
   "metadata": {},
   "source": [
    "This is the dataset on which the predictor and decoder are finetuned on. We feed the first 6 images, and let the model VQVAE model predict the next frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65608c03-b5eb-4829-9796-123a152ca660",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, T, H, W = x.shape\n",
    "\n",
    "patch_h = input_size[0] // patch_size \n",
    "patch_w = input_size[1] // patch_size\n",
    "\n",
    "# Inputting the first 6 frames as context\n",
    "with torch.no_grad():\n",
    "    z = model.encoder(x[:, :, :n_frames*tubelet_size, ...])\n",
    "\n",
    "patch_t = z.shape[1] // (patch_h * patch_w)\n",
    "z = z.reshape(B, patch_t, -1, z.shape[-1])\n",
    "z_src = z[:, : n_frames, :, :]\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a703d-c713-4cd9-b414-d9aa278a65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action encoding\n",
    "z_act = model.action_encoder(actions)\n",
    "z_act = z_act[:, : n_frames].unsqueeze(2)\n",
    "act_tiled = repeat(\n",
    "    z_act,\n",
    "    \"b t 1 a -> b t f a\",\n",
    "    f=z_src.shape[2]\n",
    ")\n",
    "\n",
    "# Latent Predictor, ViT\n",
    "# (B, num_pred, num_patches, 2)\n",
    "z_src = torch.cat([z_src, act_tiled], dim=3)\n",
    "z_src = z_src.reshape(B, -1, z.shape[-1] + 96).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a60be-81d1-442c-b362-9c7a19d24a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Predictor inference, ViT\n",
    "# (b * frames * num_patches, dim)\n",
    "with torch.no_grad():\n",
    "    z_pred = model.latent_predictor(z_src)\n",
    "z_pred = z_pred.reshape(B, n_frames, patch_h * patch_w, -1)\n",
    "z_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df477acc-ab3b-49b0-86af-246dc9488c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder to visualize the predictions\n",
    "with torch.no_grad():\n",
    "    visual_pred, diff_pred = model.decoder(\n",
    "        z_pred[..., :-action_embed_dim],\n",
    "        patch_h,\n",
    "        patch_w,\n",
    "        frames_per_latent=tubelet_size\n",
    "    )\n",
    "\n",
    "# TODO: Currently the frames are still averaged as the VAE only outputs \n",
    "# a single frame per latent.\n",
    "k = n_frames * tubelet_size\n",
    "visual_tgt = x[:, :, k :, ...].moveaxis(1, 2)\n",
    "# Reshape to (B, tubelet, C, H, W) to average tubelet size\n",
    "visual_tgt = visual_tgt.view(B, k, C, H, W)\n",
    "visual_pred = visual_pred.view(B, k, C, H, W)\n",
    "\n",
    "tgt_frames  = visual_tgt[0][-k:]\n",
    "pred_frames = visual_pred[0][-k:]\n",
    "\n",
    "T = visual_tgt[0].shape[0]  # total timesteps\n",
    "times = range(T - k, T)\n",
    "\n",
    "fig, axes = plt.subplots(2, k, figsize=(4*k, 8))\n",
    "\n",
    "for i, (t, f) in enumerate(zip(times, tgt_frames)):\n",
    "    axes[0, i].imshow(f.moveaxis(0, -1).detach().clamp(0, 1).cpu().numpy())\n",
    "    axes[0, i].set_title(f\"t = {t}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "for i, (t, f) in enumerate(zip(times, pred_frames)):\n",
    "    axes[1, i].imshow(f.moveaxis(0, -1).detach().clamp(0, 1).cpu().numpy())\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbaccfc-7e43-4e44-94e1-249b76bd2fe8",
   "metadata": {},
   "source": [
    "We can also compare the predicted latents of the source and the prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94545f2-e104-459a-b136-2aca74d95373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing everything for PCA\n",
    "embed_dim = model.encoder.embed_dim\n",
    "z_pred = z_pred[..., :-action_embed_dim].reshape(-1, embed_dim)\n",
    "\n",
    "def pca(X, n_components=3):\n",
    "    Z_mean = X.mean(0, keepdim=True)\n",
    "    Z = X - Z_mean\n",
    "    U, S, VT = torch.linalg.svd(Z, full_matrices=False)\n",
    "    \n",
    "    max_col = torch.argmax(torch.abs(U), dim=0)\n",
    "    signs = torch.sign(U[max_col, range(U.shape[1])])\n",
    "    VT *= signs[:, None]\n",
    "\n",
    "    Z = torch.matmul(Z, VT[:n_components].T)\n",
    "    return Z\n",
    "\n",
    "def min_max(X, target_min = 0.0, target_max = 1.0):\n",
    "    eps = 1e-8\n",
    "    X_std = (X - X.min(0, True).values) / (X.max(0, True).values - X.min(0, True).values + eps)\n",
    "    X_scaled = X_std * (target_max - target_min) + target_min\n",
    "    return X_scaled\n",
    "\n",
    "z_pred = min_max(pca(z_pred))\n",
    "z_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8244bf-507d-470f-9e88-a910f8be1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_target = model.encoder(x)\n",
    "\n",
    "patch_t = z_target.shape[1] // (patch_h * patch_w)\n",
    "z_target = z_target.reshape(B, patch_t, -1, embed_dim)\n",
    "\n",
    "z_target = z_target[:, n_frames:].reshape(-1, embed_dim)\n",
    "z_target = min_max(pca(z_target))\n",
    "z_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d6d9d-0e4d-43c0-859a-8f12b52ba935",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = z_pred.reshape(3, patch_h, patch_w, 3)\n",
    "gt = z_target.reshape(3, patch_h, patch_w, 3)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "for t in range(3):\n",
    "    axes[0, t].imshow(gt[t].cpu())\n",
    "    axes[0, t].set_title(f\"GT  t={t}\")\n",
    "    axes[0, t].axis(\"off\")\n",
    "\n",
    "    axes[1, t].imshow(pred[t].cpu())\n",
    "    axes[1, t].set_title(f\"Pred t={t}\")\n",
    "    axes[1, t].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf261c65-e75b-4a46-9171-53ac05e2d626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
